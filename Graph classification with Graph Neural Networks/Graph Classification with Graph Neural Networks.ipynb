{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks on Graphs\n",
    "\n",
    "The current notebook describes the process of applying classification tasks on Graphs. The models that are deployed rely on the \"Graph Networks\" framework, as defined in \"Relational Inductive Biases, Deep Learning and Graph Networks\" - [Battaglia et al. (2018)](https://arxiv.org/abs/1806.01261).\n",
    "\n",
    "The notebook is divided in four parts:\n",
    "\n",
    "#### 1. [Loading and Preprocessing](#h1)\n",
    "\n",
    "This Part includes helper functions for reading a .txt dataset file and converting it to an array of GraphDicts objects as specified in the **graph_nets** documentation. The GraphDicts format is a Python dictionary that contains all required information for the representation of the topology of a graph and its corresponding attributes on its elements.\n",
    "\n",
    "#### 2. [Defining Architectures](#h2)\n",
    "\n",
    "The used architectures take advantage of implementations that were included in the demo files of the **graph_nets** library. Most of the details of the edge, node and global update functions are reimplemented for letting in more modularity that is needed for testing different architectures. Specifically, features added are:\n",
    "\n",
    "- Customable Batch Size\n",
    "- Customable MLPs for inner update functions\n",
    "- Dropout and Skip Connections\n",
    "- Deep Architectures\n",
    "\n",
    "#### 3. [Building TensorFlow Graph](#h3)\n",
    "\n",
    "The necessary functions for building a complete training session including:\n",
    "- defining placeholders\n",
    "- loss function and optimizer\n",
    "- calculating metrics\n",
    "- utility functions such as plotting, logging, making checkpoints \n",
    "\n",
    "#### 4. [Running Classification Tasks](#h4)\n",
    "\n",
    "Classification tasks can be run either with either a nested Cross Validation, or with a Holdout set.\n",
    "\n",
    "- **Cross Validation**: Cross validation is implemented as a single function. The nested CV is comprised of an outer loop where a dataset is split into Test/TrainVal sets, and an inner loop where the TrainVal set is further split into Train and Validation sets. The Validation set is used for making checkpoints of the model. During training, information about the process is returned as a log. Each fold's training process is plotted in graphs that depict the Losses and Accuracy Score of Train and Validation sets. At the end a \".txt\" file with the confusion matrices of all folds is returned.\n",
    "\n",
    "- **Holdout Set**: A dataset is split into Train/Val/Test sets which then are used for training, optimizing and testing a model. Training with a Holdout set was used mainly for testing new architectures and implementations. \n",
    "\n",
    "\n",
    "\n",
    "### Datasets\n",
    "\n",
    "Classification is carried on the following benchmark datasets from the field of Bioinformatics\n",
    "\n",
    "- **MUTAG**,\n",
    "- **ENZYMES**,\n",
    "- **PTC**,\n",
    "\n",
    "but can be applied seamlessly to any of the datasets that are included in the corresponding github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from graph_nets import graphs\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import modules\n",
    "from graph_nets.demos import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Datasets\n",
    "\n",
    "Original source of used datasets' version was the work **\"Deep Graph Convolutional Neural Network (DGCNN)\"** of [Zhang et al. (2018)](https://github.com/muhanzhang/dgcnn). These versions of graph datasets were preferred over those included in the [TU Dortmund repository](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets) as were deemed more consistent to each other without the need for repeating basic steps of preprocessing for each different dataset. in order to leverage common graph dataset formulation for easy code reuse. \n",
    "\n",
    "#### Downloading MUTAG, ENZYMES, PTC Datasets from repository\n",
    "\n",
    "- **MUTAG**:\n",
    "https://raw.githubusercontent.com/ChNousias/graph-classification-thesis/master/datasets/MUTAG/MUTAG.txt\n",
    "\n",
    "- **ENZYMES**:\n",
    "https://raw.githubusercontent.com/ChNousias/graph-classification-thesis/master/datasets/ENZYMES/ENZYMES.txt\n",
    "\n",
    "- **PTC**:\n",
    "https://raw.githubusercontent.com/ChNousias/graph-classification-thesis/master/datasets/PTC/PTC.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MUTAG.txt', <http.client.HTTPMessage at 0x7fcf2b78a278>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If an [SSL error] occurs uncomment following 4 lines\n",
    "# import os, ssl\n",
    "# if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "#     getattr(ssl, '_create_unverified_context', None)): \n",
    "#     ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import urllib\n",
    "url = \"https://raw.githubusercontent.com/ChNousias/graph-classification-thesis/master/datasets/MUTAG/MUTAG.txt\"\n",
    "#https://raw.githubusercontent.com/ChNousias/graph-classification-thesis/master/datasets/ENZYMES/ENZYMES.txt\n",
    "#https://raw.githubusercontent.com/ChNousias/graph-classification-thesis/master/datasets/PTC/PTC.txt\n",
    "file_name = url.split('/')[-1]\n",
    "\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"h1\"></a>1. Loading and preprocessing\n",
    "\n",
    "The following set of functions include an import and preprocess action on the dataset:\n",
    "\n",
    "    - read_graph_dataset_file(f)\n",
    "    - graph_dataset_statistics(X, Y)\n",
    "    - base_graph(nodes, edges, senders, receivers, glob)\n",
    "    - one_hot(label, total)\n",
    "    - convert_targets_to_one_hot(targets, no_targets)\n",
    "    - graph_dataset_preprocessing(X, convert_attr_to_numpy)\n",
    "    \n",
    "[^](#Neural-Networks-on-Graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset, get basic statistics and plotting\n",
    "\n",
    "All bioinformatics datasets that are used in this notebook have discrete node labels, but the functions used for loading a dataset also consider node attributes that are numeric, as long as the dataset file is in the specified format as given by the [README](https://github.com/ChNousias/graph-classification-thesis/tree/master/datasets) file in the dataset section of the corresponding repository.\n",
    "\n",
    "The preprocessing function **graph_dataset_preprocessing** converts these discrete labels in one hot encodings and not further preprocessing is carried.\n",
    "\n",
    "Note that we can consider a case where a graph's attributes involve vectors that represent different objects and accompany different tags, say a vector that accompanies a node of type 1 and a vector (of the same size) that accompanies a node of type 2. Then we can convert the node tag to a one-hot vector that will replace each node tag with the rest of the vector remaining the same. \n",
    "\n",
    "That is if $v_1$ is the first node with attributes $a_1 = [1 | \\vec{x_1}]$ and $v_2$ the second node with attributes $a_2 = [2 | \\vec{x_2}]$, then we can transform the tags to the corresponding one-hot vectors leaving unchanged the accompanying numerical values, like $a_1 = [1, 0 | \\vec{x_1}]$ and $a_2 = [0, 1 | \\vec{x_2}]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph_dataset_file(f):\n",
    "    '''\n",
    "    Input: f is an opened txt file\n",
    "    \n",
    "    - first line is a header specifying number of graphs specified in txt file\n",
    "    - if line is of the form (n c) then the line specifies the start of a new graph \n",
    "        n: number of nodes\n",
    "        c: class label of new graph\n",
    "    - if line is different it specifies a new node and is of the form (t m d)\n",
    "        t: tag of node\n",
    "        m: number of neighbors followed by m values indicating neighbors indices\n",
    "        d: following d numbers indicating node's attributes\n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    total = int(next(f))\n",
    "    \n",
    "    readlines_generator = iter(f.readlines())\n",
    "    \n",
    "    build_flag = False\n",
    "    \n",
    "    for line in readlines_generator:\n",
    "        \n",
    "        parsedLine = tuple(map(float, re.findall('[0-9]+', line)))\n",
    "        \n",
    "        if len(parsedLine) == 2:\n",
    "            \n",
    "            if build_flag is True:\n",
    "                            \n",
    "                X.append(base_graph(nodes, edges, senders, receivers, glob))\n",
    "                \n",
    "                Y.append(clss)\n",
    "                \n",
    "                build_flag = False\n",
    "\n",
    "            count = 0\n",
    "            \n",
    "            graph_size, clss = tuple(map(int, parsedLine))\n",
    "            \n",
    "            nodes, edges, senders, receivers = [], [], [], []\n",
    "            \n",
    "            glob = [0]\n",
    "            \n",
    "            build_flag = True\n",
    "            \n",
    "            for k in range(graph_size):\n",
    "                \n",
    "                parsedLine = re.findall('[0-9]+', next(readlines_generator))\n",
    "            \n",
    "                tag, neighbors = tuple(map(int, parsedLine[0:2]))\n",
    "\n",
    "                edges.extend([[1.0]]*neighbors)\n",
    "\n",
    "                senders.extend([count]*neighbors)\n",
    "\n",
    "                receivers.extend(tuple(map(int, parsedLine[2: 2 + neighbors])))\n",
    "                \n",
    "                labels = []\n",
    "\n",
    "                labels.append(tag)\n",
    "                \n",
    "                labels.extend(tuple(map(float, parsedLine[2+neighbors:])))\n",
    "                \n",
    "                nodes.append(labels)\n",
    "            \n",
    "                count += 1\n",
    "                        \n",
    "    if build_flag is True:\n",
    "\n",
    "        X.append(base_graph(nodes, edges, receivers, senders, glob))\n",
    "\n",
    "        Y.append(clss)\n",
    "\n",
    "        build_flag = False\n",
    "    \n",
    "    print ('Total graphs specified: {}. Total graphs found: {}.'.format(total, len(X)))\n",
    "    \n",
    "    try:\n",
    "        assert total==len(X)\n",
    "    except AssertionError:\n",
    "        print ('Final total number of graphs is different from specified in the dataset file')\n",
    "    \n",
    "    Y = convert_targets_to_one_hot(Y)\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_graph(nodes, edges, receivers, senders, glob):\n",
    "    \"\"\"Define a basic graph structure to represent a graph\n",
    "    \n",
    "    Args:\n",
    "        nodes: a list of lists that correspond to different nodes \n",
    "        edges: a numpy array of edge attributes (of at least rank-2)\n",
    "        receivers: a list of indices that indicate receiver nodes\n",
    "        senders: a list of indices that indicate sender nodes\n",
    "        glob: a vector/tensor of global attributes\n",
    "\n",
    "    Returns:\n",
    "        data_dict: dictionary with globals, nodes, edges, receivers and senders \n",
    "                   to represent a structure like the one above.\n",
    "    \"\"\"\n",
    "    return {\n",
    "      \"globals\": glob,\n",
    "      \"nodes\": nodes,\n",
    "      \"edges\": edges,\n",
    "      \"receivers\": receivers,\n",
    "      \"senders\": senders\n",
    "    }\n",
    "\n",
    "def one_hot(label, total):\n",
    "    '''\n",
    "    Create one-hot vector.\n",
    "    \n",
    "    Args:\n",
    "        label: \n",
    "            if label == scalar:\n",
    "                a numpy array of shape (total, ) is returned\n",
    "            elif label == numpy array of discrete values and len(label)==n:\n",
    "                a numpy array of shape (n, total) is returned\n",
    "        total: number of possible discrete values\n",
    "    \n",
    "    Returns:\n",
    "        one_hot: a binary vector with the value of 1 where the target class is equal to index \n",
    "    '''\n",
    "    if isinstance(label, (int, float)):\n",
    "        one_hot = np.zeros(total, dtype = np.float32)\n",
    "        one_hot[label] = 1\n",
    "    elif isinstance(label, (list, np.ndarray)):\n",
    "        n = len(label)\n",
    "        one_hot = np.zeros((len(label), total), dtype = np.float32)\n",
    "        one_hot[np.arange(n), label] = 1\n",
    "        \n",
    "    return one_hot\n",
    "\n",
    "def convert_targets_to_one_hot(targets, no_targets = None):\n",
    "    ''' Encodes target labels to one_hot.\n",
    "    \n",
    "    Args:\n",
    "        targets: a list of target values\n",
    "        no_targets: number of unique elements in target list\n",
    "    \n",
    "    Returns:\n",
    "        one_hot_targets: targets encoded to one_hot vectors\n",
    "    '''\n",
    "    unique = list(np.unique(targets))\n",
    "    \n",
    "    if no_targets is None:\n",
    "        no_targets = len(unique)\n",
    "    \n",
    "    return one_hot([unique.index(i) for i in targets], no_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dataset_preprocessing(X_in, convert_attr_to_numpy = True, make_copy=True):\n",
    "    '''  \n",
    "    Convert dataset Node-Tag attributes to one-hot encodings:\n",
    "    \n",
    "                        t -> [0, 0, 1,.. 0]\n",
    "                        \n",
    "    In case there are extra attributes besides the tag of a node like [t | d]\n",
    "    the transformation will append node attributes at the begining of the array:\n",
    "    \n",
    "                    [t | d] -> [0, 0, 1, ... 0 | d]\n",
    "                    \n",
    "    In the case where the n available tags do not correspond to an ordered list from (0, n-1)\n",
    "    a mapping takes place which is then returned as a tokenizer for \"unseen\" examples\n",
    "    \n",
    "    Args:\n",
    "        X: graph Dataset in the GraphDicts format of graph_nets\n",
    "    Returns:\n",
    "        X_onehot: dataset with each graph's node tag attributes converted to one-hot.\n",
    "        tokenizer: for mapping specific tags to specific numbers\n",
    "    '''\n",
    "    if make_copy is True:\n",
    "        X = copy.deepcopy(X_in)\n",
    "    else:\n",
    "        X = X_in\n",
    "    \n",
    "    f = itemgetter(0)\n",
    "    tags = set()\n",
    "    for graph_item in X:\n",
    "        tags.update(f(n) for n in graph_item['nodes'])\n",
    "    \n",
    "    total_tags = len(tags)\n",
    "    \n",
    "    tokenizer = {k:v for k,v in zip(tags,range(total_tags))}\n",
    "        \n",
    "    for graph_item in X:\n",
    "        for i, node in enumerate(graph_item['nodes']):\n",
    "            label = tokenizer[f(node)]\n",
    "            graph_item['nodes'][i] = np.concatenate((one_hot(label, total_tags), node[1:]))\n",
    "            \n",
    "        if convert_attr_to_numpy is True:\n",
    "            graph_item['nodes'] = np.array(graph_item['nodes'], dtype = np.float32)\n",
    "            graph_item['edges'] = np.array(graph_item['edges'], dtype = np.float32)\n",
    "            graph_item['globals'] = np.array(graph_item['globals'], dtype = np.float32)\n",
    "        \n",
    "    return X, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_statistics(X, Y):\n",
    "    \"\"\"Get graph-dataset statistics:\n",
    "    Total # of Graphs\n",
    "    Total # of Classes\n",
    "    Total # of nodes\n",
    "    Total # of edges\n",
    "    Average # of Nodes per example\n",
    "    Average # of Edges per example\n",
    "    Edge density: Fraction of all possible edges in a graph (# edges/(# nodes)^2)*\n",
    "    * We allow directed graphs with self-loops and multiedges. Therefore we can get \n",
    "      densities bigger than 1. An \"undirected\" edge between node v and w consists of \n",
    "      two directed ones.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    total_classes = Y.shape[-1]\n",
    "    total_nodes  = sum((len(X[i]['nodes']) for i in range(n)))\n",
    "    total_edges  = sum((len(X[i]['edges']) for i in range(n)))\n",
    "    max_min_nodes = (max((len(X[i]['nodes']) for i in range(n))), \n",
    "                    min((len(X[i]['nodes']) for i in range(n))))\n",
    "    max_min_edges = (max((len(X[i]['edges']) for i in range(n))), \n",
    "                     min((len(X[i]['edges']) for i in range(n))))\n",
    "    average_nodes = total_nodes/n\n",
    "    average_edges = total_edges/n\n",
    "    edge_density = (sum(len(X[i]['edges'])/(len(X[i]['nodes']))**2 for i in range(n)))/n\n",
    "    return {\"total_graphs\":n,\n",
    "            \"total_classes\":total_classes,\n",
    "            \"total_nodes\":total_nodes,\n",
    "            \"total_edges\":total_edges,\n",
    "            \"average_nodes\":average_nodes,\n",
    "            \"average_edges\":average_edges,\n",
    "            \"max_min_nodes\":max_min_nodes,\n",
    "            \"max_min_edges\":max_min_edges, \n",
    "            \"average_edge_density\":edge_density}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify base directory where the dataset files are stored\n",
    "base_directory = 'datasets_txt_files/'\n",
    "\n",
    "datasetFile = 'MUTAG.txt'\n",
    "with open((base_directory + datasetFile), 'r') as f:\n",
    "    X,Y = read_graph_dataset_file(f)\n",
    "\n",
    "# If discrete node tags are involved convert them to one-hot encodings\n",
    "# tokenizer is a dict that stores a mapping needed for preprocessing new examples \n",
    "X, tokenizer = graph_dataset_preprocessing(X, make_copy=False)\n",
    "no_tags = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_statistics(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Visualization\n",
    "\n",
    "Graphs are visualized using the spring layout method. The color of a node depends on the corresponding tag that indicates an element of the compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_plot_set = [utils_np.data_dict_to_networkx(X[i]) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['darkred', 'orangered', 'slategrey', 'blue', 'darkslategrey', 'midnightblue',\n",
    "          'orchid', 'darkcyan', 'grey', 'dodgerblue', 'turquoise','darkviolet', 'crimson',\n",
    "          'darkorange', 'khaki', 'ivory', 'palegreen', 'limegreen', 'darkgreen', \n",
    "          'mediumseagreen', 'mediumaquamarine','teal']\n",
    "\n",
    "COLOR_DICTS = {k:colors[k] for k in range(no_tags)}\n",
    "\n",
    "if datasetFile==\"MUTAG.txt\":\n",
    "    lbl = {0:'Non-Mutagenic', 1:'Mutagenic'}\n",
    "    plot_title = 'Nitro Compounds'\n",
    "    \n",
    "elif datasetFile == \"ENZYMES.txt\":\n",
    "    lbl = {0:'EC1: Oxidoreductase', \n",
    "           1:'EC2: Transferase',\n",
    "           2:'EC3: Hydrolase', \n",
    "           3:'EC4: Lyase', \n",
    "           4:'EC5: Isomerase', \n",
    "           5:'EC6: Ligase'}\n",
    "    plot_title = 'Enzymes'\n",
    "    \n",
    "elif datasetFile == \"PTC.txt\":\n",
    "    lbl = {0: 'Non - Carcinogenic', 1:'Carcinogenic'}\n",
    "    plot_title = 'Organic Molecules'\n",
    "    \n",
    "elif datasetFile == \"PROTEINS.txt\":\n",
    "    lbl = {0: 'Enzymatic Function', 1: 'Non-Enzymatic Function'}\n",
    "    plot_title = \"Protein Structures\"\n",
    "\n",
    "def get_color_map(G, graph, no_tags):\n",
    "    color_map = []\n",
    "    Identity_Matrix = np.eye(no_tags)\n",
    "    \n",
    "    for i,node in enumerate(G):\n",
    "        color = COLOR_DICTS[np.argmax(np.all(Identity_Matrix == graph['nodes'][i], axis = 1))]\n",
    "        color_map.append(color)\n",
    "    return color_map\n",
    "\n",
    "plt.clf()\n",
    "f = plt.figure(figsize=(16,6))\n",
    "    \n",
    "for i,ind in enumerate(np.random.randint(0, len(Y), 6)):\n",
    "    ax = f.add_subplot(int(\"23\"+str(i+1)))\n",
    "    plt.title( (plot_title + ', i = {}, \\nL = {}').format(ind, lbl[np.argmax(Y[ind])]))\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    nx.draw(graph_plot_set[ind], \n",
    "            node_color = get_color_map(graph_plot_set[ind], X[ind], no_tags), \n",
    "            node_size = 140, \n",
    "            with_labels = False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"h2\"></a>2. Defining Architectures\n",
    "\n",
    "The models that are used are:\n",
    "\n",
    "1. an **\"Encoder - Core - Decoder\"** architecture which consists of:\n",
    "    - an **Encoder** and a **Decode** part that are represented by an Independent Block where all graph's elements, (edges, nodes, globals) are calculated without their relations taken into consideration, \n",
    "    - a **Core** model which consists of a Graph Neural Network model and a number of specified processing steps. Each processing step performs a \"message-passing\" action, that is, diffuses information across the graph. The output of each pass is concatenated with the initial Core input and run again \n",
    "    - The Output includes a **'Softmax'** block or said differently an Independent Block with only a global update function with target size equal to the number of target classes and a softmax as an activation function.  \n",
    "  \n",
    "  \n",
    "2. A **\"Multiple Graph Model\"** which consists of Graph Network Blocks stacked in deep architectures. At the end, a **'Softmax'** block (as degined above) is used for carrying predictions.\n",
    "  \n",
    "  \n",
    "3. An **\"Interaction Model\"** that is constrained in carrying computations on the nodes and edges of the graph omitting the use of the global block. At the end, a full GN block is used for correctly gathering the acquired information for the global block. The global block has a Softmax activation function for carrying predictions.\n",
    "\n",
    "[^](#Neural-Networks-on-Graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS FOR GN BLOCKS\n",
    "\n",
    "def MLP_model_factory(hidden_layers, output_layer,\n",
    "                      keep_rate, use_dropout = True,\n",
    "                      hidden_activation = tf.nn.relu,\n",
    "                      output_activation = tf.nn.softmax):\n",
    "    \"\"\"\n",
    "    A function factory for returning build_mlp_model functions. \n",
    "    The returned functions is evaluated in the corresponding scope of the GN:\n",
    "        EdgeBlock, NodeBlock, GlobalBlock. \n",
    "    \n",
    "    The returned function when evaluated, returns a custom sonnet MLPmodel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def build_mlp_model():\n",
    "        return MLPmodel(hidden_layers, keep_rate,\n",
    "                        use_dropout, output_layer,\n",
    "                        hidden_activation,output_activation)\n",
    "    \n",
    "    return build_mlp_model\n",
    "\n",
    "\n",
    "class MLPmodel(snt.AbstractModule):\n",
    "    \"\"\"\n",
    "    Instantiates a sonnet Module that describes a vanilla MLP model\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers, keep_rate, \n",
    "                 use_dropout = True, output_layer = None, \n",
    "                 hidden_l_non_linearity = tf.nn.relu, \n",
    "                 output_l_non_linearity = tf.nn.softmax,\n",
    "                 name = \"MLPmodel\"):\n",
    "        super(MLPmodel, self).__init__(name = name)\n",
    "        \n",
    "        self._use_dropout = use_dropout\n",
    "        self._hidden_layers = hidden_layers\n",
    "        self._output_layer = output_layer\n",
    "        \n",
    "        with self._enter_variable_scope():\n",
    "            \n",
    "            if self._hidden_layers:\n",
    "                self._hidden = [OneLayerPerceptron(layer_size, \n",
    "                                                   keep_rate,\n",
    "                                                   hidden_l_non_linearity,\n",
    "                                                   use_dropout) \n",
    "                                for layer_size in hidden_layers]\n",
    "                \n",
    "                if self._use_dropout is not True:\n",
    "                    self._layernorm = [snt.LayerNorm() \n",
    "                                        for i in range(len(self._hidden_layers))]\n",
    "                else:\n",
    "                    self._layernorm = [None for i in range(len(self._hidden_layers))]\n",
    "            \n",
    "            if self._output_layer is not None:\n",
    "                self._output = OneLayerPerceptron(self._output_layer, \n",
    "                                                  keep_rate,\n",
    "                                                  output_l_non_linearity,\n",
    "                                                  use_dropout = False)\n",
    "                \n",
    "            \n",
    "    def _build(self, inputs):\n",
    "        \"\"\"Build method for defining a full MLP layer\"\"\"\n",
    "        if self._hidden_layers:\n",
    "            f = itemgetter(0)\n",
    "            \n",
    "            latent = f(self._hidden)(inputs)\n",
    "            \n",
    "            if self._use_dropout is not True:\n",
    "                latent = f(self._layernorm)(latent)\n",
    "            \n",
    "            for hid, layernorm in zip(self._hidden[1:], self._layernorm[1:]):\n",
    "                latent = hid(latent)\n",
    "\n",
    "                if self._use_dropout is not True:\n",
    "                    latent = layernorm(latent)\n",
    "            \n",
    "        else:\n",
    "            latent = inputs\n",
    "        if self._output_layer is not None:\n",
    "            return self._output(latent)\n",
    "        else:\n",
    "            return latent\n",
    "\n",
    "\n",
    "class OneLayerPerceptron(snt.AbstractModule):\n",
    "    \"\"\"Instantiate a single_layer Perceptron with/without Dropout Layer\"\"\"\n",
    "    def __init__(self, layer_size, keep_rate,\n",
    "                 non_linearity = tf.nn.relu,\n",
    "                 use_dropout = True,\n",
    "                 name = \"OneLayerPerceptron\"):\n",
    "        super(OneLayerPerceptron, self).__init__(name = name)\n",
    "        \n",
    "        self._layer_size = layer_size\n",
    "        self._use_dropout = use_dropout\n",
    "        self._non_linearity = non_linearity\n",
    "        \n",
    "        with self._enter_variable_scope():\n",
    "            if self._use_dropout is True:\n",
    "                self._dropout = Dropout(keep_rate)\n",
    "            self._linear = snt.Linear(self._layer_size)\n",
    "        \n",
    "    def _build(self, inputs):\n",
    "        if self._use_dropout is True:\n",
    "            return self._non_linearity(self._linear(self._dropout(inputs)))\n",
    "        else:\n",
    "            return self._non_linearity(self._linear(inputs))\n",
    "\n",
    "\n",
    "class Dropout(snt.AbstractModule):\n",
    "    \"\"\"Apply Dropout as a snt.Module\"\"\"\n",
    "    def __init__(self, keep_rate, name = \"Dropout\"):\n",
    "        super(Dropout, self).__init__(name = name)\n",
    "        self._keep_rate = keep_rate\n",
    "    \n",
    "    def _build(self, inputs):\n",
    "        return tf.nn.dropout(inputs, self._keep_rate)\n",
    "\n",
    "\n",
    "class Softmax(snt.AbstractModule):\n",
    "    \"\"\"Softmax non-linearity\"\"\"\n",
    "    def __init__(self, name = \"Softmax\"):\n",
    "        super(Softmax, self).__init__(name=name)\n",
    "    \n",
    "    def _build(self, inputs):\n",
    "        return tf.nn.softmax(inputs)\n",
    "\n",
    "class MLPGraphIndependent(snt.AbstractModule):\n",
    "    \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 edge_mlp = (32, 32), node_mlp = (32, 32), global_mlp = (32, 32),\n",
    "                 edge_dropout = False, node_dropout = False, global_dropout = False,\n",
    "                 name=\"MLPGraphIndependent\"):\n",
    "        super(MLPGraphIndependent, self).__init__(name=name)\n",
    "        with self._enter_variable_scope():\n",
    "            self._network = modules.GraphIndependent(\n",
    "                  edge_model_fn=MLP_model_factory(edge_mlp, None, keep_rate, edge_dropout),\n",
    "                  node_model_fn=MLP_model_factory(node_mlp, None, keep_rate, node_dropout),\n",
    "                  global_model_fn=MLP_model_factory(global_mlp, None, keep_rate, global_dropout))\n",
    "\n",
    "    def _build(self, inputs):\n",
    "        return self._network(inputs)\n",
    "\n",
    "\n",
    "class MLPGraphNetwork(snt.AbstractModule):\n",
    "    \"\"\"GraphNetwork with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 edge_mlp = (32, 32), node_mlp = (32, 32), global_mlp = (32, 32),\n",
    "                 edge_dropout = False, node_dropout = False , global_dropout= False,\n",
    "                 name=\"MLPGraphNetwork\"):\n",
    "        super(MLPGraphNetwork, self).__init__(name=name)\n",
    "\n",
    "        with self._enter_variable_scope():\n",
    "            self._network = modules.GraphNetwork(MLP_model_factory(edge_mlp, None, keep_rate, edge_dropout), \n",
    "                                                 MLP_model_factory(node_mlp, None, keep_rate, node_dropout),\n",
    "                                                 MLP_model_factory(global_mlp, None, keep_rate, global_dropout),\n",
    "                                                 tf.unsorted_segment_sum)\n",
    "\n",
    "    def _build(self, inputs):\n",
    "        return self._network(inputs)\n",
    "    \n",
    "class MLPInteraction(snt.AbstractModule):\n",
    "    \"\"\"Interaction Networks with MLP edge and node models.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 edge_mlp = (32, 32), node_mlp = (32, 32),\n",
    "                 edge_dropout = False, node_dropout = False,\n",
    "                 name=\"MLPInteraction\"):\n",
    "        super(MLPInteraction, self).__init__(name=name)\n",
    "        with self._enter_variable_scope():\n",
    "            self._network = modules.InteractionNetwork(MLP_model_factory(edge_mlp, None, keep_rate, edge_dropout),\n",
    "                                                       MLP_model_factory(node_mlp, None, keep_rate, node_dropout),\n",
    "                                                       reducer = tf.unsorted_segment_sum)\n",
    "            \n",
    "    def _build(self, inputs):\n",
    "        return self._network(inputs)\n",
    "    \n",
    "\n",
    "class MultiGraphNetwork(snt.AbstractModule):\n",
    "    \"\"\"Multi-Graph Network is a model for stacking multiple Graph Network\n",
    "    Blocks in forward fashion with Multi-layer Perceptrons for the update\n",
    "    functions. The number of Graph Network Blocks that will be used is passed as\n",
    "    a hyper-parameter at the constructor of the model.\n",
    "    \n",
    "                *---------*     *---------*\n",
    "                |         |     |         |\n",
    "      Input --->|  #1 GN  | --->|  #2 GN  |---> ... ---> Output\n",
    "                |         |     |         |\n",
    "                *---------*     *---------*   \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                graph_block_kwargs,\n",
    "                no_graph_network_blocks = 2,\n",
    "                edge_output_size=None,\n",
    "                node_output_size=None,\n",
    "                global_output_size=None,\n",
    "                use_skip_connections = True,\n",
    "                name=\"MultiGraphNetwork\"):\n",
    "        super(MultiGraphNetwork, self).__init__(name=name)\n",
    "        self._blocks = []\n",
    "        for i in range(no_graph_network_blocks):\n",
    "            self._blocks.append(MLPGraphNetwork(**graph_block_kwargs, name = \"MLPGraphNetwork\"+\"_\"+str(i)))\n",
    "        \n",
    "        self._use_skip_connections = use_skip_connections\n",
    "        \n",
    "        # Transforms the outputs into the appropriate shapes.\n",
    "        if edge_output_size is None:\n",
    "            edge_fn = None\n",
    "        else:\n",
    "            edge_fn = lambda: snt.Linear(edge_output_size, name=\"edge_output\")\n",
    "        if node_output_size is None:\n",
    "            node_fn = None\n",
    "        else:\n",
    "            node_fn = lambda: snt.Linear(node_output_size, name=\"node_output\")\n",
    "        if global_output_size is None:\n",
    "            global_fn = None\n",
    "        else:\n",
    "            global_fn = MLP_model_factory((), global_output_size, keep_rate, False)\n",
    "        with self._enter_variable_scope():\n",
    "            self._output_transform = modules.GraphIndependent(edge_fn, node_fn,\n",
    "                                                        global_fn) \n",
    "\n",
    "    def _build(self, input_op):\n",
    "        '''Basic build method for stacking successive \n",
    "        graph network blocks'''\n",
    "        \n",
    "        latent = self._blocks[0](input_op)\n",
    "        \n",
    "        latent_skip = latent\n",
    "                \n",
    "        for i in range(1,len(self._blocks)):\n",
    "            \n",
    "            if i%2==0 and self._use_skip_connections is True:\n",
    "                                    \n",
    "#                 latent = residual_connect([self._blocks[i](latent), latent_skip])\n",
    "                \n",
    "                latent = utils_tf.concat([self._blocks[i](latent), latent_skip], axis=1)\n",
    "                \n",
    "                latent_skip = latent\n",
    "            \n",
    "            else:\n",
    "                latent = self._blocks[i](latent)\n",
    "        \n",
    "        output_ops = self._output_transform(latent)\n",
    "        \n",
    "        return output_ops\n",
    "    \n",
    "    \n",
    "class EncodeProcessDecode(snt.AbstractModule):\n",
    "    \"\"\"Full encode-process-decode model as used in graph_networks demos.\n",
    "    The model includes three components:\n",
    "    - An \"Encoder\" graph net, which independently encodes the edge, node, and\n",
    "        global attributes (does not compute relations etc.).\n",
    "    - A \"Core\" graph net, which performs N rounds of processing (message-passing)\n",
    "        steps. The input to the Core is the concatenation of the Encoder's output\n",
    "        and the previous output of the Core (labeled \"Hidden(t)\" below, where \"t\" is\n",
    "    the processing step).\n",
    "    - A \"Decoder\" graph net, which independently decodes the edge, node, and\n",
    "        global attributes (does not compute relations etc.), on each message-passing\n",
    "        step.\n",
    "                          Hidden(t)   Hidden(t+1)\n",
    "                             |            ^\n",
    "                *---------*  |  *------*  |  *---------*\n",
    "                |         |  |  |      |  |  |         |\n",
    "      Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)\n",
    "                |         |---->|      |     |         |\n",
    "                *---------*     *------*     *---------*\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                graph_block_kwargs,\n",
    "                edge_output_size=None,\n",
    "                node_output_size=None,\n",
    "                global_output_size=None,\n",
    "                name=\"EncodeProcessDecode\"):\n",
    "        super(EncodeProcessDecode, self).__init__(name=name)\n",
    "        self._encoder = MLPGraphIndependent(**graph_block_kwargs)\n",
    "        self._core = MLPGraphNetwork(**graph_block_kwargs)\n",
    "        self._decoder = MLPGraphIndependent(**graph_block_kwargs)\n",
    "        # Transforms the outputs into the appropriate shapes.\n",
    "        if edge_output_size is None:\n",
    "            edge_fn = None\n",
    "        else:\n",
    "            edge_fn = lambda: snt.Linear(edge_output_size, name=\"edge_output\")\n",
    "        if node_output_size is None:\n",
    "            node_fn = None\n",
    "        else:\n",
    "            node_fn = lambda: snt.Linear(node_output_size, name=\"node_output\")\n",
    "        if global_output_size is None:\n",
    "            global_fn = None\n",
    "        else:\n",
    "            global_fn = lambda: snt.Sequential([snt.Linear(global_output_size, name=\"global_output\"), Softmax()])\n",
    "        with self._enter_variable_scope():\n",
    "            self._output_transform = modules.GraphIndependent(edge_fn, node_fn,\n",
    "                                                        global_fn) \n",
    "            \n",
    "    def _build(self, input_op, num_processing_steps):\n",
    "        latent = self._encoder(input_op)\n",
    "        latent0 = latent\n",
    "        output_ops = []\n",
    "        for _ in range(num_processing_steps):\n",
    "            core_input = utils_tf.concat([latent0, latent], axis=1)\n",
    "            latent = self._core(core_input)\n",
    "            decoded_op = self._decoder(latent)\n",
    "            output_ops.append(self._output_transform(decoded_op))\n",
    "        return output_ops\n",
    "\n",
    "\n",
    "class InteractionModel(snt.AbstractModule):\n",
    "    \"\"\"Graph Network Interaction Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_block_kwargs,\n",
    "                 no_interaction_network_blocks = 2,\n",
    "                 edge_output_size=None,\n",
    "                 node_output_size=None,\n",
    "                 global_output_size=None,\n",
    "                 name=\"InteractionModel\"):\n",
    "        super(InteractionModel, self).__init__(name=name)\n",
    "                \n",
    "        self._blocks = []\n",
    "        for i in range(no_interaction_network_blocks):\n",
    "            self._blocks.append(MLPInteraction(**graph_block_kwargs, name = \"MLPInteraction\"+\"_\"+str(i)))\n",
    "        \n",
    "        edge_fn = MLP_model_factory(graph_block_kwargs[\"edge_mlp\"], None, keep_rate, False)\n",
    "        node_fn = MLP_model_factory(graph_block_kwargs[\"node_mlp\"], None, keep_rate, False)\n",
    "        global_fn = MLP_model_factory(graph_block_kwargs[\"node_mlp\"], global_output_size, keep_rate, False)\n",
    "        with self._enter_variable_scope():\n",
    "            self._output_transform = modules.GraphNetwork(edge_fn, \n",
    "                                                          node_fn,\n",
    "                                                          global_fn) \n",
    "\n",
    "    def _build(self, input_op):\n",
    "        n = len(self._blocks)\n",
    "        latent = self._blocks[0](input_op)\n",
    "        for i in range(1,n):\n",
    "            latent = self._blocks[i](latent)\n",
    "        output_ops = self._output_transform(latent)\n",
    "        return output_ops\n",
    "    \n",
    "class MLPCommNet(snt.AbstractModule):\n",
    "    \"\"\"Communication Network with MLP edge, node_decoder and node models.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 edge_mlp = (32,32), node_mlp = (32, 32),\n",
    "                 edge_dropout = False, node_dropout = False,\n",
    "                 name=\"MLPCommNet\"):\n",
    "        super(MLPCommNet, self).__init__(name=name)\n",
    "        with self._enter_variable_scope():\n",
    "            self._network = modules.CommNet(MLP_model_factory(edge_mlp, None, keep_rate, edge_dropout),\n",
    "                                            MLP_model_factory(edge_mlp, None, keep_rate, edge_dropout),\n",
    "                                            MLP_model_factory(node_mlp, None, keep_rate, node_dropout))\n",
    "        \n",
    "        def _build(self, inputs):\n",
    "            return self._network(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_connect(input_graphs, name=\"graph_residual\"):\n",
    "    \"\"\"Returns an operator that adds the attributes of two input graph\n",
    "    tensors along their dimension axis (axis=1).\n",
    "    \n",
    "    The purpose of this operator is to work for graph addition between two\n",
    "    same graphs in topology dimension and attribute dimension, so that if x\n",
    "    is a graph and M(x) an operator acting on a graph, we are able to retrieve:\n",
    "    \n",
    "                            Res(M(x)) = M(x) + x\n",
    "    \n",
    "    In all cases, the NODES, EDGES and GLOBALS dimensions are added\n",
    "    element-wise (if a fields is `None`, the concatenation is just a `None`), \n",
    "    therefore the the RECEIVERS, SENDERS, N_NODE and N_EDGE fields of the \n",
    "    graphs should all match.\n",
    "    \n",
    "    The graphs in `input_graphs` should also have the same set of keys for which \n",
    "    the corresponding fields is not `None`.\n",
    "    \n",
    "    Args:\n",
    "        input_graphs: A list of `graphs.GraphsTuple` objects containing `Tensor`s\n",
    "        and satisfying the constraints outlined above.\n",
    "        name: (string, optional) A name for the operation.\n",
    "    Returns: \n",
    "        An op that returns an operator that performs addition along their attribute dimension\n",
    "        for the graphs.\n",
    "    Raises:\n",
    "        ValueError if `values` is an empty list, or if the fields which are `None`\n",
    "        in `input_graphs` are not the same for all the graphs.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not input_graphs:\n",
    "        raise ValueError(\"List argument `input_graphs` is empty\")\n",
    "        \n",
    "    utils_np._check_valid_sets_of_keys([gr._asdict() for gr in input_graphs])  # pylint: disable=protected-access\n",
    "  \n",
    "    if len(input_graphs) == 1:\n",
    "        return input_graphs[0]\n",
    "\n",
    "    nodes = [gr.nodes for gr in input_graphs if gr.nodes is not None]\n",
    "    edges = [gr.edges for gr in input_graphs if gr.edges is not None]\n",
    "    globals_ = [gr.globals for gr in input_graphs if gr.globals is not None]\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        nodes = tf.add_n(nodes, name=\"add_nodes\") if nodes else None\n",
    "        edges = tf.add_n(edges, name=\"add_edges\") if edges else None\n",
    "        \n",
    "        if globals_:\n",
    "            globals_ = tf.add_n(globals_, name=\"add_globals\")\n",
    "        else:\n",
    "            globals_ = None\n",
    "        \n",
    "        output = input_graphs[0].replace(nodes=nodes, edges=edges, globals=globals_)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a name=\"h3\"></a>Building TensorFlow Graph\n",
    "\n",
    "This section is about running classification tasks on graphs, either with a k-fold Cross Validation either with a Holdout set. It involves defining beforehand several utility functions, that facilitate splitting the dataset, building the Tensorflow Graph, plotting and logging, and of course defining the main training sessions.\n",
    "\n",
    "1. **[Utility Functions for Holdout Training and Cross Validation](#Utility-Functions-for-Holdout-Training-and-Cross-Validation)**  \n",
    "2. **[Utility functions for building the TensorFlow Graph](#Utility-functions-for-building-the-TensorFlow-Graph)**\n",
    "3. **[Utility functions for Plotting and Logging](#Utility-functions-for-Plotting-and-Logging)**\n",
    "4. **[Main k-fold Cross Validation for graph classification](#Main-k-fold-Cross-Validation-for-graph-classification)**\n",
    "\n",
    "[^](#Neural-Networks-on-Graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Holdout Training and Cross Validation\n",
    "\n",
    "Some basic functions for spiting a dataset into train/val/test sets, as well as in k-folds. A stratification flag is given to ensure same distribution of classes among different folds.\n",
    "\n",
    "    - train_val_test(X, Y, splits, shuffle, stratify, seed)\n",
    "    - k_cross_validation(X, Y, folds, stratify, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(X, Y, splits, shuffle = True, stratify = True, seed = 123):\n",
    "    '''Splits and shuffles into training, validation and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X, Y: Training examples / Targets (One-hot vectors)\n",
    "        splits: tuple of train/val/test percentage splits\n",
    "        shuffle: boolean flag for shuffling dataset\n",
    "        seed: seed for random generator\n",
    "    \n",
    "    Returns:\n",
    "    X_train,Y_train, X_val, Y_val, X_test, Y_test\n",
    "    '''\n",
    "    def shuffle_data(X, Y):\n",
    "        '''Shuffle data'''\n",
    "        indices = np.random.permutation(np.arange(len(X)))\n",
    "        return ([X[i] for i in indices], np.array([Y[i] for i in indices]))\n",
    "        \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    try:\n",
    "        assert len(splits) == 3\n",
    "    except AssertionError:\n",
    "        print (\"splits list length is not equal to 3\")\n",
    "        \n",
    "    if (sum(splits) - 1.0) > 1e-06:\n",
    "        splits = np.array(splits)\n",
    "\n",
    "        splits = splits/sum(splits)\n",
    "    \n",
    "    if stratify is True:\n",
    "        strata = []\n",
    "        \n",
    "        _indices = [Y[:,i]==1 for i in range(Y.shape[-1])]\n",
    "        \n",
    "        for stratum in _indices:\n",
    "            strata.append([\n",
    "                    [X[i] for i,val in enumerate(stratum) if val==True],\n",
    "                    [Y[i] for i,val in enumerate(stratum) if val==True]\n",
    "                ])\n",
    "    else:\n",
    "        strata = [(X,Y)]\n",
    "    \n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = [], [], [], [], [], []\n",
    "    \n",
    "    for stratum in strata:\n",
    "        X_, Y_ = stratum\n",
    "        \n",
    "        n = len(X_)\n",
    "\n",
    "        if shuffle is True:\n",
    "            X_tmp, Y_tmp = shuffle_data(X_, Y_)\n",
    "        else:\n",
    "            X_tmp, Y_tmp = X_, Y_\n",
    "        \n",
    "        tr_split = int(splits[0] * n)\n",
    "\n",
    "        val_split = int((splits[0] + splits[1]) * n)\n",
    "\n",
    "        X_train.extend(X_tmp[:tr_split])\n",
    "        Y_train.extend(Y_tmp[:tr_split])\n",
    "\n",
    "        X_val.extend(X_tmp[tr_split:val_split])\n",
    "        Y_val.extend(Y_tmp[tr_split:val_split])\n",
    "\n",
    "        X_test.extend(X_tmp[val_split:])\n",
    "        Y_test.extend(Y_tmp[val_split:])\n",
    "    \n",
    "    if shuffle is True:\n",
    "        X_train, Y_train = shuffle_data(X_train, Y_train)\n",
    "        X_val, Y_val = shuffle_data(X_val, Y_val)\n",
    "        X_test, Y_test = shuffle_data(X_test, Y_test)\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_cross_validation(X, Y, folds, stratify = True, seed = 123):\n",
    "    \"\"\"Divide dataset in k folds\n",
    "    \n",
    "    Returns a list of length the number of folds with each element a \n",
    "    tuple of numpy arrays of indices corresponding to train/validation sets\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    def shuffle_array(X):\n",
    "        '''Shuffle array'''\n",
    "        indices = np.random.permutation(np.arange(len(X)))\n",
    "        return X[indices]\n",
    "            \n",
    "    if stratify is True:\n",
    "        strata = [np.flatnonzero(Y[:,i]==1) for i in range(Y.shape[-1])]\n",
    "    else:\n",
    "        strata = [np.arange(len(Y))]\n",
    "    \n",
    "    k_folds = [None for i in range(folds)] \n",
    "\n",
    "    for stratum in strata:\n",
    "        indices = shuffle_array(stratum)\n",
    "        for i, split in enumerate(np.array_split(indices, folds)):\n",
    "            if k_folds[i] is None:\n",
    "                k_folds[i] = np.stack(split)\n",
    "            else:\n",
    "                k_folds[i] = np.concatenate((k_folds[i], split))        \n",
    "    \n",
    "    return [shuffle_array(fold) for fold in k_folds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for building the TensorFlow Graph\n",
    "\n",
    "The following set of functions are facilitating the TF graph building process. They involve defining **placeholders**, calculating **loss**, **accuracy**, creating **eed_dict**, defining a **batch_size** generator:\n",
    "\n",
    "    - create_placeholders(X, Y, batch_size)\n",
    "    - create_loss_ops(targets, output_ops)\n",
    "    - compute_accuracy(targets, outputs)\n",
    "    - make_all_runnable_in_session(*args)\n",
    "    - create_feed_dict(placeholders, input_objs)\n",
    "    - graph_batch(iterable, n)\n",
    "    - check_saver(epoch, test_values, save_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(X, Y, batch_size):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation.\n",
    "\n",
    "    Args:\n",
    "    X: training examples\n",
    "    Y: labels\n",
    "    batch_size: Total number of graphs per batch.\n",
    "\n",
    "    Returns:\n",
    "    input_ph: The input graph's placeholders, as a graph namedtuple.\n",
    "    target_ph: The target graph's placeholders, as a graph namedtuple.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ph = utils_tf.placeholders_from_data_dicts(X[0:batch_size], force_dynamic_num_graphs=True)\n",
    "    \n",
    "    target_ph = tf.placeholder(tf.float32, shape =[None, Y.shape[-1]], name='labels')\n",
    "    \n",
    "    return input_ph, target_ph\n",
    "\n",
    "def create_loss_ops(targets, output_ops):\n",
    "    \"\"\"\n",
    "    Define losses based on the global output or on intermediary steps\n",
    "    \"\"\"\n",
    "    if isinstance(output_ops, list):\n",
    "        loss_ops = [tf.losses.softmax_cross_entropy(targets, output_op.globals)\n",
    "          for output_op in output_ops\n",
    "        ]        \n",
    "    else:\n",
    "        loss_ops = tf.losses.softmax_cross_entropy(targets, output_ops.globals)\n",
    "    return loss_ops\n",
    "\n",
    "def compute_accuracy(targets, outputs):\n",
    "    \"\"\"Calculate model accuracy.\n",
    "\n",
    "    Returns the number of correctly classified graphs\n",
    "\n",
    "    Args:\n",
    "    target: A numpy array that contains the target one_hot vector\n",
    "    output: A `graphs.GraphsTuple` that contains the output graph.\n",
    "\n",
    "    Returns:\n",
    "    correct: A `float` fraction of correctly labeled graphs\n",
    "    \"\"\"\n",
    "    if isinstance(outputs, list):\n",
    "        outputs_np = utils_np.graphs_tuple_to_data_dicts(outputs[-1])\n",
    "    else:\n",
    "        outputs_np = utils_np.graphs_tuple_to_data_dicts(outputs)\n",
    "    total = []\n",
    "    for target, out in zip(targets, outputs_np):\n",
    "        xn = np.argmax(target)\n",
    "        yn = np.argmax(out[\"globals\"])\n",
    "\n",
    "        total.append(xn == yn)\n",
    "\n",
    "    accuracy = np.mean(total)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "    \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]\n",
    "\n",
    "def create_feed_dict(placeholders, input_objs):\n",
    "    input_objs[0] = utils_np.data_dicts_to_graphs_tuple(input_objs[0])\n",
    "    \"\"\"Create feed_dict from a tuple/list of placeholders and input_objs\"\"\"\n",
    "    return {k:v for k,v in zip(placeholders, input_objs)}\n",
    "\n",
    "def graph_batch(iterable, n = 1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx+n, l)]\n",
    "        \n",
    "def check_saver(epoch, test_values, save_variables):\n",
    "    '''Function to check validation score\n",
    "    \n",
    "    Args:\n",
    "    iteration: current iteration\n",
    "    test_values: results from current \n",
    "    \n",
    "    Returns:\n",
    "    save_variables: Updated Saved Variables\n",
    "    '''\n",
    "    improve_flag = False\n",
    "    \n",
    "    if test_values['loss'] < save_variables['losses_best_val']:\n",
    "        \n",
    "        save_variables['losses_best_val'] = test_values['loss']\n",
    "        save_variables['improved_at'] = epoch\n",
    "        improve_flag = True\n",
    "    \n",
    "    return save_variables, improve_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for Plotting and Logging\n",
    "\n",
    "Functions for facilitating logging, plotting loss/accuracy curves and Confusion Matrices:\n",
    "    \n",
    "    - compute_log(epoch, elapsed, train_values, test_values, log_variables)\n",
    "    - predictions(targets, outputs)\n",
    "    - plot_result_curves(log_variables, fold, save, save_title)\n",
    "    - plot_confusion_matrix(cm, classes, normalize, save_title, save, title, cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log(epoch, elapsed, train_values, test_values, log_variables):\n",
    "    '''Function to update logs'''\n",
    "\n",
    "    acc_tr = compute_accuracy(train_values[\"target\"], train_values[\"outputs\"])\n",
    "    acc_val = compute_accuracy(test_values[\"target\"], test_values[\"outputs\"])\n",
    "    log_variables['last_epoch'] = epoch\n",
    "    log_variables['losses_tr'].append(train_values[\"loss\"])\n",
    "    log_variables['losses_val'].append(test_values[\"loss\"])\n",
    "    log_variables['acc_cum_tr'].append(acc_tr)\n",
    "    log_variables['acc_cum_val'].append(acc_val)\n",
    "    log_variables['logged_iterations'].append(epoch)\n",
    "\n",
    "    print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lval {:.4f}, AccTr {:.4f}, AccVal {:.4f}\".format(\n",
    "            epoch, elapsed, train_values[\"loss\"], test_values[\"loss\"],acc_tr, acc_val)) \n",
    "    \n",
    "    return log_variables\n",
    "\n",
    "        \n",
    "def predictions(targets, outputs):\n",
    "    \"\"\"Run predictions on test/val set\"\"\"\n",
    "    xn, yn = [], []\n",
    "    if isinstance(outputs, list):\n",
    "        outputs_np = utils_np.graphs_tuple_to_data_dicts(outputs[-1])\n",
    "    else:\n",
    "        outputs_np = utils_np.graphs_tuple_to_data_dicts(outputs)\n",
    "        \n",
    "    for target, out in zip(targets, outputs_np):\n",
    "        xn.append(np.argmax(target))\n",
    "        yn.append(np.argmax(out[\"globals\"]))\n",
    "    return np.array(xn), np.array(yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_curves(log_variables, fold = 0, save = True, save_title = 'GN Model'):\n",
    "    \"\"\"Function to plot loss/accuracy curves across training\"\"\"\n",
    "        \n",
    "    fig = plt.figure(1, figsize=(12, 3))\n",
    "    fig.clf()\n",
    "    x = np.array(log_variables['logged_iterations'])\n",
    "    # Loss.\n",
    "    y_tr = log_variables['losses_tr']\n",
    "    y_val = log_variables['losses_val']\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "    ax.plot(x, y_val, \"k--\", label=\"Test/generalization\")\n",
    "    ax.set_title(\"Loss across training\")\n",
    "    ax.set_xlabel(\"No of Epochs\")\n",
    "    ax.set_ylabel(\"Loss (binary cross-entropy)\")\n",
    "    ax.legend()\n",
    "    # Accuracy.\n",
    "    y_tr = log_variables['acc_cum_tr']\n",
    "    y_val = log_variables['acc_cum_val']\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "    ax.plot(x, y_val, \"k--\", label=\"Validation\")\n",
    "    ax.set_title(\"Accuracy correct across training\")\n",
    "    ax.set_xlabel(\"No of Epochs\")\n",
    "    ax.set_ylabel(\"Fraction of Graphs correctly classified\")\n",
    "    if save is True:\n",
    "        fig.savefig('plots/' + save_title + ' - Loss & Acc - fold {}.png'.format(fold), \n",
    "                    bbox_inches='tight')\n",
    "        fig.savefig('plots/' + save_title + ' - Loss & Acc - fold {}.pdf'.format(fold), \n",
    "                    bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, save_title = 'GN model', save = True,\n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues): #viridis\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else '.0f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    if save is True:\n",
    "        plt.savefig('plots/' + save_title + '.png', \n",
    "                    bbox_inches='tight')\n",
    "        plt.savefig('plots/' + save_title + '.pdf', \n",
    "                    bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main k-fold Cross Validation for graph classification\n",
    "\n",
    "Putting it all together, a function is defined for running multiple models with k-fold Cross Validation:\n",
    "\n",
    "    - get_data_from_indices(X, Y, indices)\n",
    "    - run_k_cross_validation(X_dataset, Y_dataset, outer_no_folds, inner_no_folds, \n",
    "                             num_epochs, GN_model, batch_size, log_every_epoch,\n",
    "                             NN_size, NN_dropout, dropout_keep_rate,\n",
    "                             use_skip_connections, GN_model_steps, input_optimizer, \n",
    "                             learning_rate, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_indices(X, Y, indices):\n",
    "    return [X[k] for k in indices], np.array([Y[k] for k in indices])\n",
    "\n",
    "def run_k_cross_validation(X_dataset, Y_dataset, outer_no_folds = 5, inner_no_folds = 5, \n",
    "                           num_epochs = 600, GN_model = \"Multiple_GN\", \n",
    "                           batch_size = 32, log_every_epoch = 5,\n",
    "                           NN_size = {\"edge_size\":(32, 32),\n",
    "                                      \"node_size\":(32, 32),\n",
    "                                      \"global_size\":(32, 32)},\n",
    "                           NN_dropout = {'edge_dropout':False,\n",
    "                                         'node_dropout':False,\n",
    "                                         'global_dropout':False},\n",
    "                           dropout_keep_rate = 1.0,\n",
    "                           use_skip_connections = False,\n",
    "                           GN_model_steps = 2,\n",
    "                           input_optimizer = tf.train.AdamOptimizer,\n",
    "                           learning_rate = 1e-3, seed = 123):\n",
    "    \n",
    "    outer_folds = k_cross_validation(X_dataset, Y_dataset, outer_no_folds, seed = seed + 1)\n",
    "    \n",
    "    cm_total = []\n",
    "    \n",
    "    for o in range(len(outer_folds)):\n",
    "        \n",
    "        indices_inner_train = np.concatenate([fold for j,fold in enumerate(outer_folds) if j != o])\n",
    "        indices_test = outer_folds[o]\n",
    "    \n",
    "        X, Y = get_data_from_indices(X_dataset, Y_dataset, indices_inner_train)\n",
    "        X_test, Y_test = get_data_from_indices(X_dataset, Y_dataset, indices_test)\n",
    "    \n",
    "        folds = k_cross_validation(X, Y, inner_no_folds, seed = seed + 2)\n",
    "\n",
    "\n",
    "        for i in range(len(folds)):\n",
    "            indices_train = np.concatenate([fold for j,fold in enumerate(folds) if j != i])\n",
    "            indices_val = folds[i]\n",
    "\n",
    "            X_tr, Y_tr = get_data_from_indices(X, Y, indices_train)\n",
    "            X_val, Y_val = get_data_from_indices(X, Y, indices_val)\n",
    "\n",
    "            no_clss = Y_tr.shape[-1]\n",
    "            \n",
    "            GN_graph = tf.Graph()\n",
    "            # Reset Tensorflow Graph\n",
    "            tf.reset_default_graph()\n",
    "            \n",
    "            with GN_graph.as_default():\n",
    "                # Input and target placeholders\n",
    "                input_ph, target_ph = create_placeholders(X_tr, Y_tr, batch_size)\n",
    "                global keep_rate\n",
    "                keep_rate = tf.placeholder(tf.float32, shape=(), name = \"keep_rate_ph\")\n",
    "\n",
    "                if GN_model == \"Multiple_GN\":\n",
    "                    # FORWARD GRAPH-NETWORK BLOCKS\n",
    "                    # 1. Instantiate Model\n",
    "\n",
    "                    graph_block_kwargs = {'edge_mlp': NN_size[\"edge_size\"], \n",
    "                                          'node_mlp': NN_size[\"node_size\"], \n",
    "                                          'global_mlp': NN_size[\"global_size\"],\n",
    "                                          'edge_dropout': NN_dropout['edge_dropout'],\n",
    "                                          'node_dropout': NN_dropout['node_dropout'],\n",
    "                                          'global_dropout': NN_dropout['global_dropout']\n",
    "                                          }\n",
    "\n",
    "                    GRAPH_NETWORK_OPTIONS = {'graph_block_kwargs': graph_block_kwargs,\n",
    "                                             'no_graph_network_blocks': GN_model_steps,\n",
    "                                             'use_skip_connections':use_skip_connections,\n",
    "                                             'global_output_size': no_clss}\n",
    "\n",
    "                    model =  MultiGraphNetwork(**GRAPH_NETWORK_OPTIONS)\n",
    "\n",
    "                    # 2. Build Graph - Calculate training/test loss.\n",
    "                    output_ops_tr = model(input_ph)\n",
    "                    output_ops_val = model(input_ph)\n",
    "                    loss_op_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "                    loss_op_val = create_loss_ops(target_ph, output_ops_val)\n",
    "\n",
    "                    # 3. Optimizer.\n",
    "                    optimizer = input_optimizer(learning_rate)\n",
    "                    step_op = optimizer.minimize(loss_op_tr)\n",
    "                    \n",
    "                elif GN_model == \"EncodeDecode\":\n",
    "                    # ENCODE-PROCESS-DECODE MODEL\n",
    "                    # 1. Instantiate Model\n",
    "                    graph_block_kwargs = {'edge_mlp': NN_size[\"edge_size\"], \n",
    "                                          'node_mlp': NN_size[\"node_size\"], \n",
    "                                          'global_mlp': NN_size[\"global_size\"],\n",
    "                                          'edge_dropout': NN_dropout['edge_dropout'],\n",
    "                                          'node_dropout': NN_dropout['node_dropout'],\n",
    "                                          'global_dropout': NN_dropout['global_dropout']\n",
    "                                         }\n",
    "\n",
    "                    model = EncodeProcessDecode(graph_block_kwargs, global_output_size = no_clss)\n",
    "\n",
    "                    # Number of Processing Steps in Training/Validation\n",
    "                    num_processing_steps = GN_model_steps\n",
    "\n",
    "                    # 2. Build Graph - Calculate training/test loss.\n",
    "                    output_ops_tr = model(input_ph, num_processing_steps)\n",
    "                    output_ops_val = model(input_ph, num_processing_steps)\n",
    "\n",
    "                    loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "                    loss_op_tr = sum(loss_ops_tr) / num_processing_steps\n",
    "\n",
    "                    loss_ops_val = create_loss_ops(target_ph, output_ops_val)\n",
    "                    loss_op_val = loss_ops_val[-1]\n",
    "\n",
    "                    # 3. Optimizer.\n",
    "                    optimizer = input_optimizer(learning_rate)\n",
    "                    step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "                elif GN_model == \"InteractionModel\":\n",
    "                    # INTERACTION NETWORK BLOCKS\n",
    "                    # 1. Instantiate Model\n",
    "\n",
    "                    graph_block_kwargs = {'edge_mlp': NN_size[\"edge_size\"], \n",
    "                                          'node_mlp': NN_size[\"node_size\"], \n",
    "                                          'edge_dropout': NN_dropout['edge_dropout'],\n",
    "                                          'node_dropout': NN_dropout['node_dropout']\n",
    "                                         }\n",
    "\n",
    "                    GRAPH_NETWORK_OPTIONS = {'graph_block_kwargs': graph_block_kwargs,\n",
    "                                             'no_interaction_network_blocks': GN_model_steps,\n",
    "                                             'global_output_size': no_clss}\n",
    "\n",
    "                    model =  InteractionModel(**GRAPH_NETWORK_OPTIONS)\n",
    "\n",
    "                    # 2. Build Graph - Calculate training/test loss.\n",
    "                    output_ops_tr = model(input_ph)\n",
    "                    output_ops_val = model(input_ph)\n",
    "                    loss_op_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "                    loss_op_val = create_loss_ops(target_ph, output_ops_val)\n",
    "\n",
    "                    # 3. Optimizer.\n",
    "                    optimizer = input_optimizer(learning_rate)\n",
    "                    step_op = optimizer.minimize(loss_op_tr)\n",
    "            \n",
    "            # Set current Tensorflow Session - tf.Session - tf.train.Saver\n",
    "            try:\n",
    "                sess.close()\n",
    "            except NameError:\n",
    "                pass\n",
    "\n",
    "            # Set Tensorflow to use all available cores\n",
    "            with tf.Session(graph=GN_graph ,config = tf.ConfigProto(\n",
    "                                            intra_op_parallelism_threads = 4,\n",
    "                                            inter_op_parallelism_threads = 4,\n",
    "                                            log_device_placement = False)) as sess:\n",
    "\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                log_variables = {'last_iteration':0,\n",
    "                                 'last_epoch':0,\n",
    "                                 'logged_iterations':[],\n",
    "                                 'losses_tr':  [],\n",
    "                                 'losses_val': [],\n",
    "                                 'acc_cum_tr': [],\n",
    "                                 'acc_cum_val': []}\n",
    "\n",
    "                save_variables = {'losses_best_val': 100,\n",
    "                                  'improved_at': 0,\n",
    "                                  'no_improve_limit': 100\n",
    "                }\n",
    "\n",
    "                saver = tf.train.Saver()\n",
    "                save_dir = 'checkpoints/'\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                save_path = os.path.join(save_dir, 'best_validation')\n",
    "\n",
    "                print(\"# (Epoch number), T (elapsed seconds), \"\n",
    "                      \"Ltr (training loss), Lval (validation loss), \"\n",
    "                      \"AccTr (accuracy of Training set), \"\n",
    "                      \"AccVal (accuracy of Validation set)\")\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Initialize Training Session\n",
    "\n",
    "                iteration = 0\n",
    "                train_session = {\"step\": step_op,\n",
    "                                 \"target\": target_ph,\n",
    "                                 \"loss\": loss_op_tr,\n",
    "                                 \"outputs\": output_ops_tr}\n",
    "                val_session = {\"target\": target_ph,\n",
    "                               \"loss\": loss_op_val,\n",
    "                               \"outputs\": output_ops_val}\n",
    "\n",
    "                try:\n",
    "                    for epoch in range(log_variables['last_epoch'], num_epochs):\n",
    "\n",
    "                        batch_generator = graph_batch(np.random.permutation(np.arange(len(X_tr))), batch_size)\n",
    "\n",
    "                        for batch in batch_generator:\n",
    "                            log_variables['last_iteration'] = iteration\n",
    "                            feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [\n",
    "                                                        [X_tr[ind] for ind in batch],\n",
    "                                                        [Y_tr[ind] for ind in batch],\n",
    "                                                        dropout_keep_rate])\n",
    "\n",
    "                            train_values = sess.run(train_session,feed_dict=feed_dict)\n",
    "\n",
    "                            iteration += 1\n",
    "\n",
    "                        # Calculate epoch Training and Validation Loss \n",
    "                        feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [X_tr, Y_tr, 1.00])\n",
    "                        train_values = sess.run(val_session,feed_dict=feed_dict)\n",
    "\n",
    "                        feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [X_val, Y_val, 1.00])\n",
    "                        test_values = sess.run(val_session, feed_dict=feed_dict)\n",
    "\n",
    "                        # Check log \n",
    "                        if epoch % log_every_epoch==0:\n",
    "                            elapsed = time.time() - start_time\n",
    "                            log_variables = compute_log(epoch, elapsed,train_values, \n",
    "                                                        test_values, log_variables)\n",
    "\n",
    "                        save_variables, improve_flag = check_saver(epoch, test_values, save_variables)\n",
    "\n",
    "                        if improve_flag is True:\n",
    "                            saver.save(sess=sess, save_path=save_path)\n",
    "\n",
    "                        if log_variables['last_epoch'] - save_variables['improved_at'] >= save_variables['no_improve_limit']:\n",
    "                            print (\"No improvement found for #{} epochs\".format(save_variables['no_improve_limit']))\n",
    "                            print (\"Exiting training session at epoch: #{}\".format(epoch))\n",
    "                            break\n",
    "\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"\\n\")\n",
    "                    print(\"Exiting training session at epoch: #{}\".format(log_variables['last_epoch']))\n",
    "\n",
    "                print (\"Restoring session with lowest validation Loss: \", save_variables['losses_best_val'])\n",
    "                saver.restore(sess=sess, save_path=save_path)\n",
    "\n",
    "                # Calculate Confusion Matrices by using Best model on Train/Test \n",
    "                feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [X_tr, Y_tr, 1.00])\n",
    "                train_values = sess.run(val_session, feed_dict=feed_dict)\n",
    "\n",
    "                feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [X_test, Y_test, 1.00])\n",
    "                test_values = sess.run(val_session, feed_dict=feed_dict)\n",
    "\n",
    "                if GN_model == \"Multiple_GN\":\n",
    "                    save_title = \"Multiple_GN\" + '_{}'.format(GN_model_steps)\n",
    "                elif GN_model == \"EncodeDecode\":\n",
    "                    save_title = \"EncodeDecode\" + '_{}'.format(GN_model_steps)\n",
    "                else:\n",
    "                    save_title = \"InteractionModel\" + '_{}'.format(GN_model_steps)\n",
    "\n",
    "                plot_result_curves(log_variables, fold = (o, i), save = True, save_title = save_title)\n",
    "\n",
    "                x_tr, pred_tr = predictions(train_values[\"target\"], train_values[\"outputs\"])\n",
    "                x_test, pred_test = predictions(test_values[\"target\"], test_values[\"outputs\"])\n",
    "\n",
    "                cm_tr = np.zeros((no_clss, no_clss))\n",
    "                for a, p in zip(x_tr, pred_tr):\n",
    "                    cm_tr[a][p] += 1\n",
    "\n",
    "                cm_test = np.zeros((no_clss, no_clss))\n",
    "                for a, p in zip(x_test, pred_test):\n",
    "                    cm_test[a][p] += 1\n",
    "\n",
    "                cm_total.append((cm_tr, cm_test))\n",
    "    \n",
    "    return cm_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"h4\"></a>4. Running Classification Tasks\n",
    "\n",
    "The main parts of the notebook where a training session can be defined are:\n",
    "\n",
    "#### 4A. [Training using Cross Validation](#Training-using-Cross-Validation)\n",
    "\n",
    "This part includes performing a grid search on different model parameters, training them and storing their results, that is loss/accuracy curves for each model, cumulative confusion matrices, as well as a .txt file including a confusion matrix for each fold. \n",
    "\n",
    "#### 4B. [Training Using a Holdout Set](#Training-Using-a-Holdout-Set)\n",
    "\n",
    "This part is about using a holdout set with a train/val/test split for testing new implementations.\n",
    "\n",
    "[^](#Neural-Networks-on-Graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters than can be defined are:\n",
    "\n",
    "- Graph Network Architecture choosing between:\n",
    "     - **\"Multiple GN\"**, \n",
    "     - **\"Encode - Core - Decode\"** and a \n",
    "     - **\"Interaction Model\"**\n",
    "- A model's inner update function characteristics, that is its number of layers and number of units. For each model the same number of units are used for edge, node and global update function.\n",
    "- A model's depth for the \"Multiple_GN\" and \"Interaction Model\" architecture, and recurrent steps for the \"Encode-Core-Decode\" model.\n",
    "- On which model's units (edge, block, global) dropout will be applied and with what **keep rate**.\n",
    "- Skp connections parameter.\n",
    "- Training parameters, like:\n",
    "    - **inner/outer number of folds** \n",
    "    - **learning rate**\n",
    "    - **batch size**\n",
    "    - **number of epochs**\n",
    "    \n",
    "The skip connection parameter was tested in several variants:\n",
    "\n",
    "- It was tested as a Residual connection on a graph level, where if $M(X)$ a function (a step of the learning model) that is applied on a graph, then the residual block ensures the identity function is learned, $Res(M)(x) = M(x) + x$ , that is performs an element-wise addition between the calculated block and the initial input.\n",
    "- It was tested as a skip connection of concatenation between graph's parameters. That is if $M(X)$ a function that is applied on a graph, then the skip block ensures that a part of the output skips the $M$ functions and is concatenated on the result, that is $concat([M(x), x], axis=1)$. As this could lead to a blow-up, each block's models are of the form $(2*n , n)$, where $n$ is the number of units that ensures that the model's size does not increase with more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Cross Validation\n",
    "\n",
    "Training with Cross Validation includes the following steps:\n",
    "\n",
    "- Instantiate model\n",
    "- Build Graph - Calculate training/test loss\n",
    "- Set Optimizer\n",
    "- Initialize tf.Session\n",
    "- Initialize log variables\n",
    "- Initialize tf.Saver\n",
    "- Log of training session, including **Iterations, Time, Losses, Accuracy**\n",
    "- Termination based on validation score by checking if improves over session\n",
    "- Saver to create checkpoints and restore model with best validation loss after termination\n",
    "- Calculate Confusion Matrices for training/testing sets and store them\n",
    "- Visualize Loss, Accuracy curves along training (save them to folder?)\n",
    "\n",
    "After finishing training and evaluation for each fold, function returns a list of tuples of training/testing Confusion Matrices as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Default Settings\n",
    "# Multiple_GN, InteractionModel\n",
    "# =============================\n",
    "# GN_Sizes_cv = [(16,16), (32,32), (64,64), (96,96)]\n",
    "# GN_model_steps = [2]\n",
    "# If use skip_connections choose any number of GN_model_steps (e.g. 6, 8 etc.)\n",
    "# but then must choose sizes of the form (64,32) or (48,24)\n",
    "# \n",
    "#\n",
    "# EncodeDecode\n",
    "# =============================\n",
    "# GN_Sizes_cv = [(32,32), (64,64)]\n",
    "# GN_model_steps = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GN_models_cv =  [\"Multiple_GN\", \"InteractionModel\"] #[\"Multiple_GN\", \"EncodeDecode\", \"InteractionModel\"]\n",
    "GN_Sizes_cv = [(32,32), (64,64)] \n",
    "GN_model_steps = [2]\n",
    "GN_learning_rate_cv = [1e-5]\n",
    "\n",
    "# The cartesian product for grid search or multiple training sessions\n",
    "session_cv = itertools.product(GN_models_cv, GN_Sizes_cv, GN_model_steps, GN_learning_rate_cv)\n",
    "\n",
    "for gn_model, gn_size, gn_model_step, gn_learning_rate in session_cv:\n",
    "    CV_SETTINGS = {\"outer_no_folds\":5,\n",
    "                   \"inner_no_folds\":5,\n",
    "                   \"num_epochs\": 600,\n",
    "                   \"GN_model\": gn_model,\n",
    "                   \"batch_size\": 16,\n",
    "                   \"log_every_epoch\" : 5,\n",
    "                   \"NN_size\": {\"edge_size\":gn_size,\n",
    "                               \"node_size\":gn_size,\n",
    "                               \"global_size\":gn_size},\n",
    "                   \"NN_dropout\":{\"edge_dropout\": False,\n",
    "                                 \"node_dropout\": False,\n",
    "                                 \"global_dropout\": False},\n",
    "                   \"dropout_keep_rate\":1.00,\n",
    "                   \"use_skip_connections\": False, # Applies only to Multiple_GN network\n",
    "                   \"GN_model_steps\": gn_model_step,\n",
    "                   \"input_optimizer\": tf.train.AdamOptimizer,\n",
    "                   \"learning_rate\": gn_learning_rate,\n",
    "                   \"seed\": 123}\n",
    "\n",
    "    cm_total = run_k_cross_validation(X, Y, **CV_SETTINGS)\n",
    "    \n",
    "    # Extract some information for plots' titles and saved files\n",
    "    title_cm = CV_SETTINGS[\"GN_model\"] + '_{}'.format(CV_SETTINGS[\"GN_model_steps\"])\n",
    "    title_folds = '[{}x{}]'.format(str(CV_SETTINGS[\"outer_no_folds\"]),\\\n",
    "                                   str(CV_SETTINGS[\"inner_no_folds\"]))\n",
    "    title_NN = '[{}N]'.format(CV_SETTINGS[\"NN_size\"]['node_size'])\n",
    "    title_LR = '[{}L]'.format(CV_SETTINGS[\"learning_rate\"])\n",
    "    title_BS = '[{}BS]'.format(CV_SETTINGS[\"batch_size\"])\n",
    "\n",
    "    title_report = '-'.join([title_cm, title_folds, title_NN, title_LR, title_BS])\n",
    "    plots_cm_path = 'plots/Confusion_Matrices-'\n",
    "    path = plots_cm_path + title_report + '.txt'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('np.' + np.array(cm_total).__repr__())\n",
    "\n",
    "    cm_total_cumulative = np.sum(np.array(cm_total), axis = 0)\n",
    "\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm_total_cumulative[0], classes=[0,1], save_title = title_report+'-CMTrainingSet', \n",
    "                          save = True, title='Cumulative Confusion matrix of 5x5-fold Training Sets')\n",
    "\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm_total_cumulative[1], classes=[0,1], save_title = title_report+'-CMTestingSet', \n",
    "                          save = True, title='Cumulative Confusion matrix of 5x5-fold Testing Sets')\n",
    "\n",
    "    # Transfer Output Files to Training Output Files Directory\n",
    "    current_dir = os.path.join(os.getcwd(), 'plots')\n",
    "    target_dir = os.path.join(current_dir, title_report)\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    session_files = os.listdir(current_dir)\n",
    "\n",
    "    for f in session_files:\n",
    "        if f.endswith(('.txt', '.pdf', '.png')):\n",
    "            shutil.move(os.path.join(current_dir, f), target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Using a Holdout Set\n",
    "\n",
    "Similar to Cross Validation training with a Holdout Set includes the following steps:\n",
    "\n",
    "- Instantiate model\n",
    "- Build Graph - Calculate training/test loss\n",
    "- Set Optimizer\n",
    "- Initialize tf.Session\n",
    "- Initialize log variables\n",
    "- Initialize tf.Saver\n",
    "- Log of training session, including **Iterations, Time, Losses, Accuracy**\n",
    "- Termination based on validation score by checking if improves over session\n",
    "- Saver to create checkpoints and restore model with best validation loss after termination\n",
    "- Calculate Confusion Matrices for training/testing sets and store them\n",
    "- Visualize Loss, Accuracy curves along training\n",
    "\n",
    "After finishing training and evaluation, training/testing Confusion Matrices as numpy arrays are calculated for visualization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining training Architecture\n",
    "\n",
    "After splitting a dataset in some target sets for defining a training architecture, use the corresponding cell.\n",
    "\n",
    "1. #### Encode - Process - Decode Model\n",
    "2. #### Multiple Graph Network Model\n",
    "3. #### Interaction Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = train_val_test(X, Y, splits = (0.7,0.15,0.15), stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reset Tensorflow Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data / training parameters.\n",
    "num_epochs = 600\n",
    "batch_size_tr = 8\n",
    "batch_size_val = Y_val.shape[0]\n",
    "\n",
    "# Data.\n",
    "# Input and target placeholders\n",
    "input_ph, target_ph = create_placeholders(X_train, Y_train, batch_size_tr)\n",
    "keep_rate = tf.placeholder(tf.float32, shape=(), name = \"keep_rate_ph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encode - Process - Decode Model\n",
    "\n",
    "Parameters to change:\n",
    "\n",
    "- num_processing_steps on training and validation graph (can be different)\n",
    "- edge/node/global MLP parameters\n",
    "- edge/node/global dropout enable\n",
    "- learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1- ENCODE-PROCESS-DECODE MODEL\n",
    "\n",
    "# Model parameters - Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 5\n",
    "num_processing_steps_val = 5\n",
    "\n",
    "# 1. Instantiate Model\n",
    "graph_block_kwargs = {'edge_mlp': (32, 32), \n",
    "                      'node_mlp': (32, 32), \n",
    "                      'global_mlp': (32, 32),\n",
    "                      'edge_dropout': False,\n",
    "                      'node_dropout': False,\n",
    "                      'global_dropout': False\n",
    "                     }\n",
    "\n",
    "model = EncodeProcessDecode(graph_block_kwargs, global_output_size = Y_train.shape[-1])\n",
    "\n",
    "# To add Dropout add is_training flag to distinguish between training_validation\n",
    "\n",
    "# 2. Build Graph - Calculate training/test loss.\n",
    "output_ops_tr = model(input_ph, num_processing_steps_tr)\n",
    "output_ops_val = model(input_ph, num_processing_steps_val)\n",
    "\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "\n",
    "loss_ops_val = create_loss_ops(target_ph, output_ops_val)\n",
    "loss_op_val = loss_ops_val[-1]\n",
    "\n",
    "# 3. Optimizer.\n",
    "learning_rate = 1e-5\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward Graph Network Blocks Model\n",
    "\n",
    "Parameters to change:\n",
    "\n",
    "- no of graph network blocks\n",
    "- edge/node/global MLP parameters\n",
    "- edge/node/global dropout enable\n",
    "- enable skip connections between Multiple GN blocks\n",
    "- learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -2- FORWARD GRAPH-NETWORK BLOCKS\n",
    "\n",
    "# 1. Instantiate Model\n",
    "\n",
    "graph_block_kwargs = {'edge_mlp': (32, 32), \n",
    "                      'node_mlp': (32, 32), \n",
    "                      'global_mlp': (32, 32),\n",
    "                      'edge_dropout': False,\n",
    "                      'node_dropout': False,\n",
    "                      'global_dropout': False\n",
    "                     }\n",
    "\n",
    "GRAPH_NETWORK_OPTIONS = {'graph_block_kwargs': graph_block_kwargs,\n",
    "                         'no_graph_network_blocks': 4,\n",
    "                         'use_skip_connections':True,\n",
    "                         'global_output_size': Y_train.shape[-1]}\n",
    "\n",
    "model =  MultiGraphNetwork(**GRAPH_NETWORK_OPTIONS)\n",
    "\n",
    "# 2. Build Graph - Calculate training/test loss.\n",
    "output_ops_tr = model(input_ph)\n",
    "output_ops_val = model(input_ph)\n",
    "loss_op_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "loss_op_val = create_loss_ops(target_ph, output_ops_val)\n",
    "\n",
    "# 3. Optimizer.\n",
    "learning_rate = 3e-5\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_parameters = 0\n",
    "# for variable in tf.trainable_variables():\n",
    "#     # shape is an array of tf.Dimension\n",
    "#     shape = variable.get_shape()\n",
    "# #     print(shape)\n",
    "# #     print(len(shape))\n",
    "#     variable_parameters = 1\n",
    "#     for dim in shape:\n",
    "# #         print(dim)\n",
    "#         variable_parameters *= dim.value\n",
    "#     print(variable_parameters)\n",
    "#     total_parameters += variable_parameters\n",
    "# print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interaction Model\n",
    "\n",
    "- no of interaction network blocks\n",
    "- edge/node MLP parameters\n",
    "- edge/node dropout enable\n",
    "- learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -3- INTERACTION NETWORK BLOCKS\n",
    "# 1. Instantiate Model\n",
    "\n",
    "graph_block_kwargs = {'edge_mlp': (32, 32), \n",
    "                      'node_mlp': (32, 32), \n",
    "                      'edge_dropout': False,\n",
    "                      'node_dropout': False\n",
    "                     }\n",
    "\n",
    "GRAPH_NETWORK_OPTIONS = {'graph_block_kwargs': graph_block_kwargs,\n",
    "                         'no_interaction_network_blocks': 2,\n",
    "                         'global_output_size': Y_train.shape[-1]}\n",
    "\n",
    "model =  InteractionModel(**GRAPH_NETWORK_OPTIONS)\n",
    "\n",
    "# 2. Build Graph - Calculate training/test loss.\n",
    "output_ops_tr = model(input_ph)\n",
    "output_ops_val = model(input_ph)\n",
    "loss_op_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "loss_op_val = create_loss_ops(target_ph, output_ops_val)\n",
    "\n",
    "# 3. Optimizer.\n",
    "learning_rate = 3e-5\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Graph initialization\n",
    "\n",
    "- Initializie tensorflow graph\n",
    "- Specify threads to be used\n",
    "- Initialize Global Variables\n",
    "- Initialize Log Variables\n",
    "- Initialize Saver and Checkpoint Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell resets the Tensorflow session, but keeps the same computational graph\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "sess = tf.Session(config = tf.ConfigProto(intra_op_parallelism_threads = 4,\n",
    "                                          inter_op_parallelism_threads = 4,\n",
    "                                          log_device_placement = False))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "log_variables = {'last_iteration':0,\n",
    "                 'last_epoch':0,\n",
    "                 'logged_iterations':[],\n",
    "                 'losses_tr':  [],\n",
    "                 'losses_val': [],\n",
    "                 'acc_cum_tr': [],\n",
    "                 'acc_cum_val': []}\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "\n",
    "save_variables = {'losses_best_val': 100,\n",
    "                  'improved_at': 0,\n",
    "                  'no_improve_limit': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Network training session\n",
    "\n",
    "Training session includes:\n",
    "- log of training session, including **Iterations, Time, Losses, Accuracy**\n",
    "- termination based on validation score by checking if improves over session\n",
    "- saver to create checkpoints and restore model with best val loss after termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"# (Epoch number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lval (test/generalization loss), \"\n",
    "      \"AccTr (accuracy of training set), \"\n",
    "      \"AccVal (accuracy of test set)\")\n",
    "\n",
    "start_time = time.time()\n",
    "iteration = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(log_variables['last_epoch'], num_epochs):\n",
    "        \n",
    "        batch_generator = graph_batch(np.random.permutation(np.arange(len(X_train))), batch_size_tr)\n",
    "        \n",
    "        for batch in batch_generator:\n",
    "            \n",
    "            log_variables['last_iteration'] = iteration\n",
    "            feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [\n",
    "                                        [X_train[ind] for ind in batch],\n",
    "                                        [Y_train[ind] for ind in batch],\n",
    "                                        1.00])\n",
    "            train_values = sess.run({\n",
    "              \"step\": step_op,\n",
    "              \"target\": target_ph,\n",
    "              \"loss\": loss_op_tr,\n",
    "              \"outputs\": output_ops_tr\n",
    "            },\n",
    "                feed_dict=feed_dict)\n",
    "            \n",
    "            iteration += 1\n",
    "\n",
    "        feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [X_train, Y_train, 1.00])\n",
    "        test_values = sess.run({\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_val,\n",
    "            \"outputs\": output_ops_val\n",
    "        },\n",
    "            feed_dict=feed_dict)\n",
    "            \n",
    "        feed_dict=create_feed_dict([input_ph, target_ph, keep_rate], [X_val, Y_val, 1.00])\n",
    "        test_values = sess.run({\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_val,\n",
    "            \"outputs\": output_ops_val\n",
    "        },\n",
    "            feed_dict=feed_dict)\n",
    "\n",
    "        if epoch%5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            log_variables = compute_log(epoch, elapsed, train_values, test_values, log_variables)\n",
    "\n",
    "        save_variables, improve_flag = check_saver(epoch, test_values, save_variables)\n",
    "\n",
    "        if improve_flag is True:\n",
    "            saver.save(sess=sess, save_path=save_path)\n",
    "\n",
    "        if log_variables['last_epoch'] - save_variables['improved_at'] >= save_variables['no_improve_limit']:\n",
    "            print (\"No improvement found for #{} iterations\".format(save_variables['no_improve_limit']))\n",
    "            print (\"Restoring session with lowest validation Loss: \", save_variables['losses_best_val'])\n",
    "            saver.restore(sess=sess, save_path=save_path)\n",
    "            print (\"Exiting training session at iteration number: #{}\".format(iteration))\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\")\n",
    "    print(\"Exiting training session at epoch number: #{}\".format(log_variables['last_epoch']))\n",
    "    print (\"Restoring session with lowest validation Loss: \", save_variables['losses_best_val'])\n",
    "    saver.restore(sess=sess, save_path=save_path)\n",
    "    \n",
    "print (\"Restoring session with lowest validation Loss: \", save_variables['losses_best_val'])\n",
    "saver.restore(sess=sess, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result_curves(log_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, pred_tr = predictions(train_values[\"target\"], train_values[\"outputs\"])\n",
    "x_val, pred_val = predictions(test_values[\"target\"], test_values[\"outputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_labels = Y_train.shape[-1]\n",
    "\n",
    "cm_tr = np.zeros((no_of_labels,no_of_labels))\n",
    "for a, p in zip(x_tr, pred_tr):\n",
    "    cm_tr[a][p] += 1\n",
    "    \n",
    "cm_val = np.zeros((no_of_labels,no_of_labels))\n",
    "for a, p in zip(x_val, pred_val):\n",
    "    cm_val[a][p] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cm_tr, classes=list(range(no_of_labels)), save_title = 'Multiple_GN_2', save = False,\n",
    "                      title='Cumulative Confusion matrix of 10-fold Training Sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
